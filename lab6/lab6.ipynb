{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laboratorium 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hiro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/hiro/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "from wikipedia.exceptions import WikipediaException\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "from typing import List, Dict\n",
    "import string\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from scipy import sparse\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tekst artukułu jest wyszukiwany i pobierany za pomocą biblioteki wikipedia. Po otrzymaniu danego artykułu jest on zapisywany do folderu ```articles``` za pomocą blbioteki ```pickle```."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Załadownie n-pierwszych artykułów z enwiki dump. (Jest to potrzebne ponieważ plik, który zainstalowałem ma ponad 300 000 tytułów artykułu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleLoader:\n",
    "    def __init__(self, n=10**4, file='articles/enwiki-pages-articles.txt'):\n",
    "        self.n = n\n",
    "        self.file = file\n",
    "        self.enwiki_name = 'enwiki-pages-articles.txt'\n",
    "        self.titles = []\n",
    "        self.dict_of_key_words = dict()\n",
    "    # zadanie 1 - przygotowanie zbioru dokumentów tekstowych\n",
    "    def get_names_of_n_first_articles(self):\n",
    "        with open(self.file, 'r',encoding=\"utf8\") as f:\n",
    "            csv_reader = csv.reader(f, delimiter=':')\n",
    "            i = 0\n",
    "            for row in csv_reader:\n",
    "                if i >= self.n:\n",
    "                    break\n",
    "\n",
    "                title_of_article = row[-1]\n",
    "            \n",
    "                if '/' in title_of_article:\n",
    "                    continue\n",
    "\n",
    "                self.titles.append(title_of_article)\n",
    "                i += 1\n",
    "\n",
    "        random.shuffle(self.titles)\n",
    "    \n",
    "    def load_and_save_articles(self):\n",
    "        not_found = 0\n",
    "        if not self.titles:\n",
    "            self.get_names_of_n_first_articles()\n",
    "\n",
    "        for title in self.titles:\n",
    "            try:\n",
    "                wiki_page = wikipedia.page(title)\n",
    "                url = wiki_page.url\n",
    "                content = wiki_page.content\n",
    "                dict_of_title = {'content': content, 'url': url}\n",
    "                pickle.dump(dict_of_title, open('articles/{}'.format(title), 'wb'))\n",
    "            except WikipediaException:\n",
    "                not_found += 1\n",
    "            except Exception as e:\n",
    "                print(\"exception ocured {}\".format(str(e)))\n",
    "        print(\"Titles not found: {}\".format(not_found))\n",
    "\n",
    "    # zadabie 2 - określenie słownika słów kluczowych -> będzie to słwonik z kluczem jako nazwa artykułu, a wartością jako tekst artykułu\n",
    "    \n",
    "    # stworzenie nowego słownika dla słów kluczowych\n",
    "    def create_dict_of_key_words(self, length = 1000):\n",
    "        i = 0\n",
    "        # self.dict_of_key_words[\"6\"] = pickle.load(open(\"articles/{}\".format(6), \"rb\"))\n",
    "\n",
    "        for title in self.pickle_yeild():\n",
    "            if i > length: break\n",
    "            if title == self.enwiki_name or \"key_words\" in title: \n",
    "                continue\n",
    "            \n",
    "            self.dict_of_key_words[title] = pickle.load(open(\"articles/{}\".format(title), \"rb\"))\n",
    "            i+=1\n",
    "\n",
    "\n",
    "        pickle.dump(self.dict_of_key_words, open('articles/dumped_data/dict_of_key_words_{}'.format(len(self.dict_of_key_words)), 'wb'))\n",
    "    \n",
    "    def pickle_yeild(self, path = 'articles'):\n",
    "        for content in os.listdir(path):\n",
    "            if os.path.isfile(os.path.join(path, content)):\n",
    "                yield content\n",
    "\n",
    "    # wczytanie wcześniej stworzonego słownika dla słów kluczowych\n",
    "    def load_dict_of_key_words(self, dirpath = 'articles/dumped_data', dumped_file = 'dict_of_key_words_1001'):\n",
    "        path = os.path.join(dirpath, dumped_file)\n",
    "        self.dict_of_key_words = pickle.load(open(path, \"rb\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_handler = ArticleLoader(n= 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_handler.get_names_of_n_first_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hiro/.local/lib/python3.10/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/hiro/.local/lib/python3.10/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles not found: 125\n"
     ]
    }
   ],
   "source": [
    "article_handler.load_and_save_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_handler.create_dict_of_key_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_handler.load_dict_of_key_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dennis Joseph Enright OBE FRSL (11 March 1920 – 31 December 2002) was a British academic, poet, novelist and critic. He authored Academic Year (1955), Memoirs of a Mendicant Professor (1969) and a wide range of essays, reviews, anthologies, children's books and poems.\n",
      "\n",
      "\n",
      "== Life ==\n",
      "Enright was born in Royal Leamington Spa, Warwickshire, to Irish postman father George Enright - a former soldier, \"obliged in early life to enlist... as the result of the premature death of his father, a Fenian\" - and Welsh chapel-goer mother Grace (née Cleaver); he wrote about his \"working-class, Black Country upbringing\". Enright stated in his poem \"Anglo-Irish\" that his \"father claimed to be descended from a king called Brian Boru, an ancient hero of Ireland...\" but his \"mother said that all Irishmen claimed descent from kings but the truth was they were Catholics.\" His early life was characterised by poverty, the loss of his father, and relationship with his \"overworked mother\". He was educated at Leamington College and Downing College, Cambridge. After graduating he held a number of academic posts outside the United Kingdom: in Egypt, Japan, Thailand and notably in Singapore (from 1960). He at times attributed his lack of success in finding a post closer to home to writing for Scrutiny and his short association with F. R. Leavis; whose influence he mainly and early, but not entirely, rejected.\n",
      "As a poet he was identified with the Movement. His 1955 anthology, Poets of the 1950s, served to delineate the group of British poets in question – albeit somewhat remotely and retrospectively, since he was abroad and it was not as prominent as the Robert Conquest collection New Lines of the following year.\n",
      "Returning to London in 1970, he edited Encounter magazine, with Melvin J. Lasky, for two years. He subsequently worked in publishing.\n",
      "\n",
      "\n",
      "== The \"Enright Affair\" ==\n",
      "Enright gained some notoriety in Singapore after his inaugural lecture at the University of Singapore on 17 November 1960, titled \"Robert Graves and the Decline of Modernism\". His introductory remarks on the state of culture in Singapore were the subject of a Straits Times article. \"'Hands Off' Challenge to 'Culture Vultures'\", the next day. Among other things, he stated that it was important for Singapore and Malaya to remain \"culturally open\", that culture was something to be left for the people to build up, and that for the government to institute \"a sarong culture, complete with pantun competitions and so forth\" was futile.Some quotes include:\n",
      "\n",
      "Art does not begin in a test-tube, it does not take its origin in good sentiments and clean-shaven, upstanding young thoughts.\n",
      "Leave the people free to make their own mistakes, to suffer and to discover. Authority must leave us to fight even that deadly battle over whether or not to enter a place of entertainment wherein lurks a juke-box, and whether or not to slip a coin into the machine.\n",
      "The following day, Enright was summoned by the Ministry for Labour and Law regarding his foreigner work permit, and was handed a letter by the Minister for Culture, S. Rajaratnam, which had also been released to the press. The letter admonished Enright for \"involv[ing] [himself] in political affairs which are the concern of local people\", not \"visitors, including mendicant professors\", and said that the government \"[has] no time for asinine sneers by passing aliens about the futility of 'sarong culture complete with pantun competitions' particularly when it comes from beatnik professors\". There was also some criticism that Enright had been insensitive towards Malays and their so-called \"sarong culture\".\n",
      "With some mediation from the Academic Staff Association of the university, it was agreed that to put the matter to rest, Enright would write a letter of apology and clarification, the government would reply, and both were to be printed in the newspapers. Although the affair was \"essentially dead\" after that, according to Enright, it would still be brought up periodically in discussions of local culture and academic freedom. Enright gave his account of the incident in Memoirs of a Mendicant Professor (pp. 124–151).\n",
      "\n",
      "\n",
      "== Timeline ==\n",
      "11 March 1920: Born in Warwickshire\n",
      "1947–50: Lecturer in English, University of Alexandria\n",
      "1950–53: Organising Tutor, Extra-Mural Department, Birmingham University\n",
      "1953–56: Visiting Professor, Konan University, Japan\n",
      "1956–67: Visiting Lecturer, Free University of Berlin\n",
      "1967–69: British Council Professor, Chulalongkorn University\n",
      "1960–70: Professor of English, University of Singapore\n",
      "1970–72: Co-Editor, Encounter\n",
      "1974–82: Director, Chatto & Windus\n",
      "1975–80: Honorary Professor of English, University of Warwick\n",
      "1981: Queen's Gold Medal for Poetry\n",
      "1991: OBE\n",
      "31 December 2002: Died in London\n",
      "\n",
      "\n",
      "== Bibliography ==\n",
      "\n",
      "\n",
      "=== Poetry ===\n",
      "Collections\n",
      "\n",
      "The Laughing Hyena and other poems (1953)\n",
      "Bread Rather than Blossoms (1956), poems\n",
      "The Year of the Monkey (1956), poems\n",
      "Some Men Are Brothers (1960), poems\n",
      "Addictions (1962), poems\n",
      "The Old Adam (1965)\n",
      "Selected Poems (1968)\n",
      "Unlawful Assembly (1968)\n",
      "Daughters of Earth (1972), poems\n",
      "Foreign Devils (1972), poems\n",
      "The Terrible Shears – Scenes from a Twenties Childhood (1973)\n",
      "Sad Ires (1975), poems\n",
      "Paradise Illustrated (1978), poems (translated into Dutch by C. Buddingh' in 1982 as Het paradijs in beeld in a bilingual edition)\n",
      "A Faust Book (1979), poems\n",
      "Collected Poems (1981)\n",
      "Collected Poems 1987\n",
      "Selected Poems 1990, Oxford\n",
      "Under the Circumstances: Poems and Prose (1991)\n",
      "Old Men and Comets (1993) poems\n",
      "Collected Poems: 1948–1998 (1998)Anthologies (edited)\n",
      "\n",
      "Poets of the 1950s (1955)\n",
      "The Poetry of Living Japan (1958), editor with Takamichi Ninomiya\n",
      "A Choice of Milton's Verse (1975) editor\n",
      "Penguin Modern Poets 26 (1975) with Dannie Abse and Michael Longley\n",
      "The Oxford Book of Contemporary Verse 1945–1980 (1980), editorList of poems\n",
      "\n",
      "In the Basilica of the Annunciation (1971), broadsheet poem\n",
      "The Rebel (1974), poem\n",
      "Walking in the Harz Mountains, Faust Senses the Presence of God (1979), poem\n",
      "\n",
      "\n",
      "=== Novels ===\n",
      "Academic Year (1955)\n",
      "Heaven Knows Where (1957)\n",
      "Insufficient Poppy (1960)\n",
      "Figures of Speech (1965 )\n",
      "The Joke Shop (1976), novel\n",
      "Wild Ghost Chase (1978), novel\n",
      "Beyond Land's End (1979), novel\n",
      "\n",
      "\n",
      "=== Literary criticism, memoirs and general anthologies ===\n",
      "A Commentary on Goethe's Faust (1949), translated into Polish by Bohdan Zadura:Ksiega Fausta, Wydawnictwo Lubelskie, Lublin 1984.\n",
      "The World of Dew: Aspects of Living Japan (1955)\n",
      "The Apothecary's Shop (1957)\n",
      "Robert Graves and the Decline of Modernism (1960)\n",
      "English Critical Texts 16th Century to 20th Century (1963), editor with Ernst de Chickera\n",
      "Conspirators and Poets (1966)\n",
      "Memoirs of a Mendicant Professor (1969)\n",
      "Shakespeare and the Students (1970)\n",
      "Man is an Onion: Reviews and Essays (1972)\n",
      "Rhyme times rhyme (1974)\n",
      "A Mania for Sentences: Essays on G. Grass, H. Boll, Frisch, Flaubert & Others (1983)\n",
      "Fair of Speech: The Uses of Euphemism (1985), editor\n",
      "Instant Chronicles: A Life (1985)\n",
      "The Oxford Book of Death (1985), editor\n",
      "The Alluring Problem – An Essay on Irony (1986)\n",
      "Fields of Vision: Essays on Literature, Language, and Television (1988)\n",
      "Ill at Ease: Writers on Ailments Real and Imagined (1989), editor\n",
      "The Faber Book of Fevers and Frets (1989), editor\n",
      "Oxford Book of Friendship (1991), editor with David Rawlinson\n",
      "The Way of The Cat (1992)\n",
      "The Oxford Book of the Supernatural (1994), editor\n",
      "Interplay: A Kind of Commonplace Book (1995)\n",
      "Telling Tales (1999)\n",
      "Play Resumed: A Journal\n",
      "Signs and Wonders: Selected Essays (2001)\n",
      "Injury Time (2003)\n",
      "\n",
      "\n",
      "== Notes ==\n",
      "\n",
      "\n",
      "== References ==\n",
      "William Walsh (1974): D. J. Enright: Poet of Humanism, Cambridge University Press, ISBN 978-0521203838\n",
      "Jacqueline Simms (editor; 1990). Life By Other Means. Essays on D. J. Enright, Oxford University Press, ISBN 978-0192129895\n",
      "\n",
      "\n",
      "== External links ==\n",
      "\"D.J. Enright\", Fellows Remembered, The Royal Society of Literature\n",
      "D. J. Enright at Library of Congress, with 77 library catalogue records (previous page of browse report mainly, as 'Enright, D. J. (Dennis Joseph), 1920–' without '2002')\n",
      "\"D.J. Enright\", The Rebel Analysis, (W3train – A Project of Hajvery Institute)\n"
     ]
    }
   ],
   "source": [
    "print(article_handler.dict_of_key_words['D. J. Enright']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleSearchEngine(object):\n",
    "    def __init__(self, articles_key_words : Dict[str, Dict[str, str]]):\n",
    "        self.articles = articles_key_words\n",
    "        self.titles = list(self.articles.keys())\n",
    "        self.occured_words = defaultdict(lambda: 0)\n",
    "        self.uniq_occured_words = None\n",
    "        self.ids_of_unique_occured_word = None\n",
    "        self.parsed_article = dict()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatisation = WordNetLemmatizer()\n",
    "        self.term_by_document_matrix = None\n",
    "        self.matrix_without_noise = None\n",
    "\n",
    "\n",
    "    def parse_article_text(self, text: str, if_article = True):                     # dzięki if_article możemy sparsować dostosować parsowanie tekstu w zależności od czy jest artykułem czy nie\n",
    "        parse_text = text.lower()                                                   # nie ma sensu w odróżnianiu małych i dużych liter\n",
    "        parse_text = re.sub('['+string.punctuation+']', '', parse_text).split()     # parsowanie po tekście w celu usunięcia niepotrzebnych znajów np. #$%...\n",
    "        parsed_text = [word for word in parse_text if word not in self.stop_words]  # usuwanie stop_wordów np. 'ours', 'i' ...\n",
    "\n",
    "        words_dictonary = defaultdict(lambda: 0)                                    # dla nie powstałego słownika przypisz 0 jako wartość\n",
    "        words_counter = 0                                                           # liczenie ilości słów\n",
    "        for word in parsed_text:\n",
    "            lemma_word = self.lemmatisation.lemmatize(word, pos='v')              # pos = 'v' - verb lemmatisation np. goes -> go ...\n",
    "            words_dictonary[lemma_word] += 1                                        # dzięki defaultdict mamy pewność, że jak nie istnieje dany klucz to stworzy się nowy z wartością 0\n",
    "            words_counter += 1\n",
    "            if if_article:\n",
    "                self.occured_words[lemma_word] += 1\n",
    "        return dict(words_dictonary = words_dictonary, words_counter = words_counter)\n",
    "            \n",
    "\n",
    "    def article_parser(self):\n",
    "        for article_title in self.titles:\n",
    "            self.parsed_article[article_title] = dict()\n",
    "            self.parsed_article[article_title]['article_content'] = self.parse_article_text(self.articles[article_title]['content'])\n",
    "\n",
    "        self.uniq_occured_words = list(self.occured_words.keys())\n",
    "        self.ids_of_unique_occured_word = {self.uniq_occured_words[i]: i for i in range(len(self.uniq_occured_words))}\n",
    "\n",
    "\n",
    "    def create_bags_of_words(self):\n",
    "        for article_title in self.titles:\n",
    "            self.parsed_article[article_title]['bag_of_words'] = self.fill_bag_of_words(self.parsed_article[article_title]['article_content'])\n",
    "    \n",
    "\n",
    "    def fill_bag_of_words(self, article_content):\n",
    "        article_words = article_content['words_dictonary'].keys()\n",
    "\n",
    "        # tworzenie macierzy rzadkiej -> wymiary: ilość wszystkich unikalnych słów we wszystkich tekstów / 1 -> jeden, ponieważ działamy na jednym artykule w danym momencie\n",
    "        # dok_matrix -> Dictionary Of Keys based sparse matrix, efficient structure for constructing sparse matrices incrementally\n",
    "        vector = sparse.dok_matrix(np.zeros((len(self.ids_of_unique_occured_word), 1))) \n",
    "        for word_key in article_words:\n",
    "            vector[self.ids_of_unique_occured_word[word_key], 0] = article_content['words_dictonary'][word_key]\n",
    "\n",
    "        vector /= article_content['words_counter'] # normalizacja wektora -> dzielenie przez ilość unikalnych słów w artykule\n",
    "        return sparse.csr_matrix(vector) # csr_matrix -> Compressed Sparse Row matrix\n",
    "    \n",
    "    \n",
    "    def create_term_by_document_matrix(self):\n",
    "        n = len(self.titles)\n",
    "        m = len(self.uniq_occured_words)\n",
    "        self.term_by_document_matrix = sparse.lil_matrix((m, n)) # m jest liczbą termów w słowniku, n liczbą dokumentów\n",
    "        for i in range(n):\n",
    "            self.term_by_document_matrix[:,i] = self.parsed_article[self.titles[i]]['bag_of_words']\n",
    "        # utworzenie skompresowanej macierzy rzadkiej by operacje arytmetyczny były wmiarę czasowo wykonalane\n",
    "        self.term_by_document_matrix = sparse.csr_matrix(self.term_by_document_matrix)\n",
    "\n",
    "\n",
    "    def multiply_by_IDF(self):\n",
    "        n = len(self.titles)\n",
    "        for word in self.uniq_occured_words:\n",
    "            # nw jest liczbą dokumentów, w których występuje słowo w\n",
    "            nw = sum(1 for article in self.parsed_article.values() if word in article['article_content']['words_dictonary'])\n",
    "            id_of_word = self.ids_of_unique_occured_word[word]\n",
    "            idf_word = np.log(n/nw)\n",
    "            self.term_by_document_matrix[id_of_word] *= idf_word\n",
    "    \n",
    "\n",
    "    def normalize_vector(self, vector):\n",
    "        return np.sqrt((vector.power(2)).sum())\n",
    "\n",
    "\n",
    "    def find_best_articles(self, input_words, num_of_articles_to_return = 10, with_noise = True):\n",
    "        matrix = self.term_by_document_matrix if with_noise else self.matrix_without_noise\n",
    "\n",
    "        parsed_input_words = self.parse_article_text(input_words, if_article = False)\n",
    "        q = self.fill_bag_of_words(parsed_input_words)\n",
    "        q_norm = self.normalize_vector(q)\n",
    "        probs_of_article = [] # prawdopodobieństwo dla wystąpienia artykułu\n",
    "        for i in range(len(self.titles)):\n",
    "            d_i = matrix.getcol(i)\n",
    "            d_i_norm = self.normalize_vector(d_i)\n",
    "            cos_theta_i = (q.T @ d_i) / (q_norm * d_i_norm)\n",
    "            probs_of_article.append((cos_theta_i, i)) # prawdopodowbieństwo oraz miejsce wystąpienia artykułu\n",
    "        # return: search_link, tytuł, wartość prawdopodobieństwa\n",
    "        probs_of_article.sort(key = lambda x: x[0], reverse = True)\n",
    "        \n",
    "        for prob, idx in probs_of_article[:num_of_articles_to_return]:\n",
    "            found_article_title = self.titles[idx]\n",
    "            found_article_url = self.articles[found_article_title]['url']\n",
    "            print(\"url: {} title: {} probability: {}\".format(found_article_url, found_article_title, prob))\n",
    "    \n",
    "    def normalize_vectors(self):\n",
    "        for i in range(len(self.titles)):\n",
    "            vector = self.term_by_document_matrix.getcol(i)\n",
    "            vector_norm = self.normalize_vector(vector)\n",
    "            self.term_by_document_matrix[:, i] /= vector_norm\n",
    "\n",
    "    def create_search_engine1(self):\n",
    "        self.article_parser()\n",
    "    def create_search_engine2(self):\n",
    "        self.create_bags_of_words()\n",
    "    def create_search_engine3(self):\n",
    "        self.create_term_by_document_matrix()\n",
    "    def create_search_engine4(self):\n",
    "        self.multiply_by_IDF()\n",
    "    def create_search_engine5(self):\n",
    "        self.normalize_vectors()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (1, 0)\t0.0005361930294906167\n",
      "  (3, 0)\t0.0018766756032171583\n",
      "  (4, 0)\t0.0005361930294906167\n",
      "  (6, 0)\t0.0008042895442359249\n",
      "  (7, 0)\t0.0008042895442359249\n",
      "  (12, 0)\t0.0010723860589812334\n",
      "  (13, 0)\t0.0016085790884718498\n",
      "  (15, 0)\t0.00026809651474530834\n",
      "  (16, 0)\t0.0021447721179624667\n",
      "  (17, 0)\t0.00026809651474530834\n",
      "  (19, 0)\t0.00026809651474530834\n",
      "  (21, 0)\t0.00026809651474530834\n",
      "  (23, 0)\t0.0016085790884718498\n",
      "  (24, 0)\t0.00026809651474530834\n",
      "  (25, 0)\t0.0005361930294906167\n",
      "  (27, 0)\t0.00026809651474530834\n",
      "  (28, 0)\t0.0005361930294906167\n",
      "  (38, 0)\t0.00026809651474530834\n",
      "  (42, 0)\t0.00026809651474530834\n",
      "  (44, 0)\t0.0010723860589812334\n",
      "  (45, 0)\t0.00026809651474530834\n",
      "  (46, 0)\t0.0008042895442359249\n",
      "  (48, 0)\t0.00026809651474530834\n",
      "  (51, 0)\t0.0010723860589812334\n",
      "  (52, 0)\t0.00026809651474530834\n",
      "  :\t:\n",
      "  (187677, 0)\t0.00026809651474530834\n",
      "  (187678, 0)\t0.00026809651474530834\n",
      "  (187679, 0)\t0.00026809651474530834\n",
      "  (187680, 0)\t0.00026809651474530834\n",
      "  (187681, 0)\t0.0013404825737265416\n",
      "  (187682, 0)\t0.00026809651474530834\n",
      "  (187683, 0)\t0.00026809651474530834\n",
      "  (187684, 0)\t0.00026809651474530834\n",
      "  (187685, 0)\t0.00026809651474530834\n",
      "  (187686, 0)\t0.00026809651474530834\n",
      "  (187687, 0)\t0.00026809651474530834\n",
      "  (187688, 0)\t0.00026809651474530834\n",
      "  (187689, 0)\t0.00026809651474530834\n",
      "  (187690, 0)\t0.00026809651474530834\n",
      "  (187691, 0)\t0.00026809651474530834\n",
      "  (187692, 0)\t0.00026809651474530834\n",
      "  (187693, 0)\t0.00026809651474530834\n",
      "  (187694, 0)\t0.00026809651474530834\n",
      "  (187695, 0)\t0.00026809651474530834\n",
      "  (187696, 0)\t0.00026809651474530834\n",
      "  (187697, 0)\t0.0005361930294906167\n",
      "  (187698, 0)\t0.00026809651474530834\n",
      "  (187699, 0)\t0.00026809651474530834\n",
      "  (187700, 0)\t0.00026809651474530834\n",
      "  (187701, 0)\t0.00026809651474530834\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(article_engine.parsed_article['Alma Aty']['bag_of_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "716"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_engine.parsed_article[' A New Spelling of My Name']['article_content']['words_counter']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTOWANIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_engine = ArticleSearchEngine(article_handler.dict_of_key_words)\n",
    "article_engine.create_search_engine1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_engine.create_search_engine2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_engine.create_search_engine3()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_engine.create_search_engine4()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<93591x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 176 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_engine.term_by_document_matrix.getcol(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<93591x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 136 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_engine.create_search_engine5()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: https://en.wikipedia.org/wiki/Meningitis_Now title: Meningitis Trust probability:   (0, 0)\t0.6827337428128849\n",
      "url: https://en.wikipedia.org/wiki/Phonological_history_of_English_vowels title: Vane–vain–vein mergers probability:   (0, 0)\t0.13077090848181608\n",
      "url: https://en.wikipedia.org/wiki/Carbon_Trust title: Carbon Trust probability:   (0, 0)\t0.09699294832593375\n",
      "url: https://en.wikipedia.org/wiki/Fight_for_Sight_(UK) title: Iris Fund for Prevention of Blindness probability:   (0, 0)\t0.08675845950882263\n",
      "url: https://en.wikipedia.org/wiki/Municipal_mergers_and_dissolutions_in_Japan title: Municipal mergers and dissolutions in Japan probability:   (0, 0)\t0.07834557180797601\n"
     ]
    }
   ],
   "source": [
    "article_engine.find_best_articles(\"frombork\", num_of_articles_to_return = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audre lorde grows up in harlem in the 1930s and 1940s, a child of black west indian parents. lorde is legally blind from a very young age, isolating her even further from her surroundings and a family from which she does not receive much warmth or affection. her two older sisters, phyllis and helen, are very close, but are rarely mentioned in zami and lorde spends little time with them. her parents and other adults, especially her mother, discipline her harshly for insolence. lorde\n",
      "audre lorde grows up in harlem in the 1930s and 1940s a child of black west indian parents lorde is legally blind from a very young age isolating her even further from her surroundings and a family from which she does not receive much warmth or affection her two older sisters phyllis and helen are very close but are rarely mentioned in zami and lorde spends little time with them her parents and other adults especially her mother discipline her harshly for insolence lorde\n",
      "audre lorde grows up in harlem in the s and s a child of black west indian parents lorde is legally blind from a very young age isolating her even further from her surroundings and a family from which she does not receive much warmth or affection her two older sisters phyllis and helen are very close but are rarely mentioned in zami and lorde spends little time with them her parents and other adults especially her mother discipline her harshly for insolence lorde\n",
      "audre lorde grows up in harlem in the s and s a child of black west indian parents lorde is legally blind from a very young age isolating her even further from her surroundings and a family from which she does not receive much warmth or affection her two older sisters phyllis and helen are very close but are rarely mentioned in zami and lorde spends little time with them her parents and other adults especially her mother discipline her harshly for insolence lorde\n",
      "audre lorde grows harlem s s child black west indian parents lorde legally blind young age isolating surroundings family receive warmth affection older sisters phyllis helen close rarely mentioned zami lorde spends little time parents adults especially mother discipline harshly insolence lorde\n",
      "['audre', 'lorde', 'grows', 'up', 'in', 'harlem', 'in', 'the', '1930s', 'and', '1940s', 'a', 'child', 'of', 'black', 'west', 'indian', 'parents', 'lorde', 'is', 'legally', 'blind', 'from', 'a', 'very', 'young', 'age', 'isolating', 'her', 'even', 'further', 'from', 'her', 'surroundings', 'and', 'a', 'family', 'from', 'which', 'she', 'does', 'not', 'receive', 'much', 'warmth', 'or', 'affection', 'her', 'two', 'older', 'sisters', 'phyllis', 'and', 'helen', 'are', 'very', 'close', 'but', 'are', 'rarely', 'mentioned', 'in', 'zami', 'and', 'lorde', 'spends', 'little', 'time', 'with', 'them', 'her', 'parents', 'and', 'other', 'adults', 'especially', 'her', 'mother', 'discipline', 'her', 'harshly', 'for', 'insolence', 'lorde']\n",
      "['audre', 'lorde', 'grows', 'harlem', '1930s', '1940s', 'child', 'black', 'west', 'indian', 'parents', 'lorde', 'legally', 'blind', 'young', 'age', 'isolating', 'even', 'surroundings', 'family', 'receive', 'much', 'warmth', 'affection', 'two', 'older', 'sisters', 'phyllis', 'helen', 'close', 'rarely', 'mentioned', 'zami', 'lorde', 'spends', 'little', 'time', 'parents', 'adults', 'especially', 'mother', 'discipline', 'harshly', 'insolence', 'lorde']\n",
      "['audre', 'lorde', 'grows', 'harlem', 's', 's', 'child', 'black', 'west', 'indian', 'parents', 'lorde', 'legally', 'blind', 'young', 'age', 'isolating', 'surroundings', 'family', 'receive', 'warmth', 'affection', 'older', 'sisters', 'phyllis', 'helen', 'close', 'rarely', 'mentioned', 'zami', 'lorde', 'spends', 'little', 'time', 'parents', 'adults', 'especially', 'mother', 'discipline', 'harshly', 'insolence', 'lorde']\n",
      "42\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "content = \"Audre Lorde grows up in Harlem in the 1930s and 1940s, a child of Black West Indian parents. Lorde is legally blind from a very young age, isolating her even further from her surroundings and a family from which she does not receive much warmth or affection. Her two older sisters, Phyllis and Helen, are very close, but are rarely mentioned in Zami and Lorde spends little time with them. Her parents and other adults, especially her mother, discipline her harshly for insolence. Lorde\"\n",
    "\n",
    "content = content.lower()\n",
    "content2 = content.lower()\n",
    "print(content)\n",
    "content = re.sub(r'[^\\w\\s]', '', content)\n",
    "print(content)\n",
    "content = re.sub('[0-9]', '', content)\n",
    "print(content)\n",
    "content = re.sub(' {2} +',' ', content)\n",
    "print(content)\n",
    "content = remove_stopwords(content)\n",
    "print(content)\n",
    "\n",
    "# parsowanie stringów\n",
    "content2 = re.sub('['+string.punctuation+']', '', content2).split()\n",
    "print(content2)\n",
    "\n",
    "# usuowanie stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "content2re = [word for word in content2 if word not in stop_words]\n",
    "\n",
    "\n",
    "print(content2re)\n",
    "print(content.split())\n",
    "print(len(content.split()))\n",
    "print(len(content2re))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
