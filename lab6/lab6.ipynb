{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laboratorium 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hiro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/hiro/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "from wikipedia.exceptions import WikipediaException\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "from typing import List, Dict\n",
    "import string\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from scipy import sparse\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tekst artukułu jest wyszukiwany i pobierany za pomocą biblioteki wikipedia. Po otrzymaniu danego artykułu jest on zapisywany do folderu ```articles``` za pomocą blbioteki ```pickle```."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Załadownie n-pierwszych artykułów z enwiki dump. (Jest to potrzebne ponieważ plik, który zainstalowałem ma ponad 300 000 tytułów artykułu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleLoader:\n",
    "    def __init__(self, n=10**4, file='articles/enwiki-pages-articles.txt'):\n",
    "        self.n = n\n",
    "        self.file = file\n",
    "        self.enwiki_name = 'enwiki-pages-articles.txt'\n",
    "        self.titles = []\n",
    "        self.dict_of_key_words = dict()\n",
    "    # zadanie 1 - przygotowanie zbioru dokumentów tekstowych\n",
    "    def get_names_of_n_first_articles(self):\n",
    "        with open(self.file, 'r',encoding=\"utf8\") as f:\n",
    "            csv_reader = csv.reader(f, delimiter=':')\n",
    "            i = 0\n",
    "            for row in csv_reader:\n",
    "                if i >= self.n:\n",
    "                    break\n",
    "\n",
    "                title_of_article = row[-1]\n",
    "            \n",
    "                if '/' in title_of_article:\n",
    "                    continue\n",
    "\n",
    "                self.titles.append(title_of_article)\n",
    "                i += 1\n",
    "\n",
    "        random.shuffle(self.titles)\n",
    "    \n",
    "    def load_and_save_articles(self):\n",
    "        not_found = 0\n",
    "        if not self.titles:\n",
    "            self.get_names_of_n_first_articles()\n",
    "\n",
    "        for title in self.titles:\n",
    "            try:\n",
    "                wiki_page = wikipedia.page(title)\n",
    "                url = wiki_page.url\n",
    "                content = wiki_page.content\n",
    "                dict_of_title = {'content': content, 'url': url}\n",
    "                pickle.dump(dict_of_title, open('articles/{}'.format(title), 'wb'))\n",
    "            except WikipediaException:\n",
    "                not_found += 1\n",
    "            except Exception as e:\n",
    "                print(\"exception ocured {}\".format(str(e)))\n",
    "        print(\"Titles not found: {}\".format(not_found))\n",
    "\n",
    "    # zadabie 2 - określenie słownika słów kluczowych -> będzie to słwonik z kluczem jako nazwa artykułu, a wartością jako tekst artykułu\n",
    "    \n",
    "    # stworzenie nowego słownika dla słów kluczowych\n",
    "    def create_dict_of_key_words(self, length = 1000):\n",
    "        i = 0\n",
    "        # self.dict_of_key_words[\"6\"] = pickle.load(open(\"articles/{}\".format(6), \"rb\"))\n",
    "\n",
    "        for title in self.pickle_yeild():\n",
    "            if i > length: break\n",
    "            if title == self.enwiki_name or \"key_words\" in title: \n",
    "                continue\n",
    "            \n",
    "            self.dict_of_key_words[title] = pickle.load(open(\"articles/{}\".format(title), \"rb\"))\n",
    "            i+=1\n",
    "\n",
    "\n",
    "        pickle.dump(self.dict_of_key_words, open('articles/dumped_data/dict_of_key_words_{}'.format(len(self.dict_of_key_words)), 'wb'))\n",
    "    \n",
    "    def pickle_yeild(self, path = 'articles'):\n",
    "        for content in os.listdir(path):\n",
    "            if os.path.isfile(os.path.join(path, content)):\n",
    "                yield content\n",
    "\n",
    "    # wczytanie wcześniej stworzonego słownika dla słów kluczowych\n",
    "    def load_dict_of_key_words(self, dirpath = 'articles/dumped_data', dumped_file = 'dict_of_key_words_1001'):\n",
    "        path = os.path.join(dirpath, dumped_file)\n",
    "        self.dict_of_key_words = pickle.load(open(path, \"rb\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stworzenie klasy ArticleLoader dla n elementów, pozwala ona na zarządzanie danymi z wikipedii. Można wczytać, zaspisać lub odczytać tytuły razem z linkami do tytułu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_handler = ArticleLoader(n= 1000)\n",
    "article_handler.get_names_of_n_first_articles()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wywołanie funkcji ```load_and_save_articles``` pozwala na przeszukanie wikipedii i zapisanie zawartości do pliku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hiro/.local/lib/python3.10/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/hiro/.local/lib/python3.10/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles not found: 125\n"
     ]
    }
   ],
   "source": [
    "article_handler.load_and_save_articles()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wywołanie funkcji ```create_dict_of_key_words``` pozwala na stworzenie słownika, w którym można odczytać zawartość pliku odnosząc się do tytułu artukułu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_handler.create_dict_of_key_words()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wywołanie funkcji ```load_dict_of_key_words``` wczytuje do słownika wcześniej zapisany słownik -> pomocne gdyż tworzenie słownika zajmuje bardzo dużo czasu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_handler.load_dict_of_key_words()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Klasa ```ArticleSearchEngine``` -> tworzenie programu wyszukującego artykuły"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicjalizacja klasy ```ArticleSearchEngine``` przez słownik z kluczami jako tytuły oraz z wartościami jako słownik dla danego tytułu zawierającego wartość tekstową artykułu oraz link url"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klasa ta wykonuje kroki podane w treści zadań z laboratorium\n",
    "- ```parse_article_text``` oraz ```article_parser```: parsowanie artykułu - zlicza ilości występowania danych wyrazów - jest to unia wszystkich wyrazów, które wystąpiły we wszystkuch artykułach oraz tworzy słownik z identyfikatorami, który jest pomocny w dalszej części klasy do zarządzania macierzami rzadkimi\n",
    "- ```create_bags_of_words``` oraz ```fill_bag_of_words```: tworzenie *bag_of_words* - początkowo jest tworzona macierz rzadka o długości ilości wszystkich unikalnych słów. Następnie każda komurka jest wypełniana ilością wystąpienia słowa w danym artykule. Jest to operacja, która jest powtarzana dla każdego artykułu.\n",
    "- ```create_term_by_document_matrix```: tworzona jest macierz *term-by-document matrix* w której wektory cech ułożone są kolumnowo (wektory -> odpowidni *bag_of_words*)\n",
    "- ```multiply_by_IDF```: wstępne przetwarzanie otrzymanego zbioru elementy *bag_of_words* są mnożone przez *inverse document frequency*\n",
    "- ```find_best_articles```: pozwala wprowadzać zapytania w postaci sekwencji słów, oraz zwraca *k* najbardziej trafnych dokumentów. Kożysta ona z funkcji ```parse_article_text```, by można było zawęzić zbiór przeszukiwać oraz dostosować wprowadzone zapytanie do przyjętej konwencji zapisywania słów. (np. goes zamieni się na go)\n",
    "- ```normalize_vectors```: normalizuje wszystkie wektory w macierzy *term_by_document*.\n",
    "- ```apply_SVD_and_LRA```: usuwa szum z macierzy *term_by_document* i jest jest stosowane low rank aproximation. Nowe miary prawdopodobieństwa są obliczane w funkcji ```find_best_articles```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleSearchEngine(object):\n",
    "    def __init__(self, articles_key_words : Dict[str, Dict[str, str]]):\n",
    "        self.articles = articles_key_words\n",
    "        self.titles = list(self.articles.keys())\n",
    "        self.occured_words = defaultdict(lambda: 0)\n",
    "        self.uniq_occured_words = None\n",
    "        self.ids_of_unique_occured_word = None\n",
    "        self.parsed_article = dict()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatisation = WordNetLemmatizer()\n",
    "        self.term_by_document_matrix = None                 # macierz rzadka przechowująca wektory cech\n",
    "        self.matrix_without_noise = None                    # przechowuje macierz po zastosowaniu SVD\n",
    "\n",
    "\n",
    "    def parse_article_text(self, text: str, if_article = True):                     # dzięki if_article możemy sparsować dostosować parsowanie tekstu w zależności od czy jest artykułem czy nie\n",
    "        parse_text = text.lower()                                                   # nie ma sensu w odróżnianiu małych i dużych liter\n",
    "        parse_text = re.sub('['+string.punctuation+']', '', parse_text).split()     # parsowanie po tekście w celu usunięcia niepotrzebnych znajów np. #$%...\n",
    "        parsed_text = [word for word in parse_text if word not in self.stop_words]  # usuwanie stop_wordów np. 'ours', 'i' ...\n",
    "\n",
    "        words_dictonary = defaultdict(lambda: 0)                                    # dla nie powstałego słownika przypisz 0 jako wartość\n",
    "        words_counter = 0                                                           # liczenie ilości słów\n",
    "        for word in parsed_text:\n",
    "            lemma_word = self.lemmatisation.lemmatize(word, pos='v')                # pos = 'v' - verb lemmatisation np. goes -> go ...\n",
    "            words_dictonary[lemma_word] += 1                                        # dzięki defaultdict mamy pewność, że jak nie istnieje dany klucz to stworzy się nowy z wartością 0\n",
    "            words_counter += 1\n",
    "            if if_article:\n",
    "                self.occured_words[lemma_word] += 1\n",
    "        return dict(words_dictonary = words_dictonary, words_counter = words_counter)\n",
    "            \n",
    "\n",
    "    def article_parser(self):\n",
    "        for article_title in self.titles:\n",
    "            self.parsed_article[article_title] = dict()\n",
    "            self.parsed_article[article_title]['article_content'] = self.parse_article_text(self.articles[article_title]['content'])\n",
    "\n",
    "        self.uniq_occured_words = list(self.occured_words.keys())\n",
    "        self.ids_of_unique_occured_word = {self.uniq_occured_words[i]: i for i in range(len(self.uniq_occured_words))}\n",
    "\n",
    "\n",
    "    def create_bags_of_words(self):\n",
    "        for article_title in self.titles:\n",
    "            self.parsed_article[article_title]['bag_of_words'] = self.fill_bag_of_words(self.parsed_article[article_title]['article_content'])\n",
    "    \n",
    "\n",
    "    def fill_bag_of_words(self, article_content):\n",
    "        article_words = article_content['words_dictonary'].keys()\n",
    "\n",
    "        # tworzenie macierzy rzadkiej -> wymiary: ilość wszystkich unikalnych słów we wszystkich tekstów / 1 -> jeden, ponieważ działamy na jednym artykule w danym momencie\n",
    "        # dok_matrix -> Dictionary Of Keys based sparse matrix, efficient structure for constructing sparse matrices incrementally\n",
    "        vector = sparse.dok_matrix(np.zeros((len(self.ids_of_unique_occured_word), 1))) \n",
    "        for word_key in article_words:\n",
    "            vector[self.ids_of_unique_occured_word[word_key], 0] = article_content['words_dictonary'][word_key]\n",
    "\n",
    "        vector /= article_content['words_counter'] # normalizacja wektora -> dzielenie przez ilość unikalnych słów w artykule\n",
    "        return sparse.csr_matrix(vector) # csr_matrix -> Compressed Sparse Row matrix\n",
    "    \n",
    "    \n",
    "    def create_term_by_document_matrix(self):\n",
    "        # n - ilość artukułów, m - ilość unikalnych słów w wszystkich tekstach\n",
    "        n = len(self.titles)\n",
    "        m = len(self.uniq_occured_words)\n",
    "        self.term_by_document_matrix = sparse.lil_matrix((m, n)) # m jest liczbą termów w słowniku, n liczbą dokumentów\n",
    "        for i in range(n):\n",
    "            self.term_by_document_matrix[:,i] = self.parsed_article[self.titles[i]]['bag_of_words']\n",
    "        # utworzenie skompresowanej macierzy rzadkiej by operacje arytmetyczny były w miarę czasowo wykonalane\n",
    "        self.term_by_document_matrix = sparse.csr_matrix(self.term_by_document_matrix)\n",
    "\n",
    "\n",
    "    def multiply_by_IDF(self):\n",
    "        n = len(self.titles)\n",
    "        for word in self.uniq_occured_words:\n",
    "            # nw jest liczbą dokumentów, w których występuje słowo w\n",
    "            nw = sum(1 for article in self.parsed_article.values() if word in article['article_content']['words_dictonary'])\n",
    "            id_of_word = self.ids_of_unique_occured_word[word]\n",
    "            idf_word = np.log(n/nw)\n",
    "            self.term_by_document_matrix[id_of_word] *= idf_word\n",
    "    \n",
    "\n",
    "    def normalize_vector(self, vector):\n",
    "        return np.sqrt((vector.power(2)).sum())\n",
    "\n",
    "\n",
    "    def find_best_articles(self, input_words, num_of_articles_to_return = 10, with_noise = True):\n",
    "        matrix = self.term_by_document_matrix if with_noise else self.matrix_without_noise\n",
    "\n",
    "        parsed_input_words = self.parse_article_text(input_words, if_article = False)\n",
    "        q = self.fill_bag_of_words(parsed_input_words)\n",
    "        q_norm = self.normalize_vector(q)\n",
    "        probs_of_article = [] # prawdopodobieństwo dla wystąpienia artykułu\n",
    "        for i in range(len(self.titles)):\n",
    "            d_i = matrix.getcol(i)\n",
    "            d_i_norm = self.normalize_vector(d_i)\n",
    "            cos_theta_i = (q.T @ d_i) / (q_norm * d_i_norm)\n",
    "            probs_of_article.append((cos_theta_i, i)) # prawdopodowbieństwo oraz miejsce wystąpienia artykułu\n",
    "        # return: search_link, tytuł, wartość prawdopodobieństwa\n",
    "        probs_of_article.sort(key = lambda x: x[0], reverse = True)\n",
    "        \n",
    "        for prob, idx in probs_of_article[:num_of_articles_to_return]:\n",
    "            found_article_title = self.titles[idx]\n",
    "            found_article_url = self.articles[found_article_title]['url']\n",
    "            print(\"url: {} \\ttitle: {} \\tprobability: {}\".format(found_article_url, found_article_title, prob[(0,0)]))\n",
    "    \n",
    "    # normalizacja wszystkich wektorów wektorów\n",
    "    def normalize_vectors(self):\n",
    "        for i in range(len(self.titles)):\n",
    "            vector = self.term_by_document_matrix.getcol(i)\n",
    "            vector_norm = self.normalize_vector(vector)\n",
    "            self.term_by_document_matrix[:, i] /= vector_norm\n",
    "\n",
    "\n",
    "    def apply_SVD_and_LRA(self, k = 100):\n",
    "        # zastosowanie svds ze względu na podanie k, a self.term_by_document_matrix jest macierzą rzadką więc chcemy wziąć k pierwszych wartości\n",
    "        u, s, vh = sparse.linalg.svds(self.term_by_document_matrix, k=k)\n",
    "        # utworzenie macierzy rzadkich z svd dla żądanego k\n",
    "        u = sparse.csr_matrix(u)\n",
    "        s = sparse.diags(s)\n",
    "        vh = sparse.csr_matrix(vh)\n",
    "        self.matrix_without_noise = u @ s @ vh # macierz po usunięciu szumów\n",
    "\n",
    "    # stworzenie wielu metod do testowania następnych kroków wyszukiwarki\n",
    "    def create_search_engine1(self):\n",
    "        self.article_parser()\n",
    "    def create_search_engine2(self):\n",
    "        self.create_bags_of_words()\n",
    "    def create_search_engine3(self):\n",
    "        self.create_term_by_document_matrix()\n",
    "    def create_search_engine4(self):\n",
    "        self.multiply_by_IDF()\n",
    "    def create_search_engine5(self):\n",
    "        self.normalize_vectors()\n",
    "    \n",
    "    def create_search_engine(self):\n",
    "        self.article_parser()\n",
    "        self.create_bags_of_words()\n",
    "        self.create_term_by_document_matrix()\n",
    "        self.multiply_by_IDF()\n",
    "        self.normalize_vectors()\n",
    "        self.apply_SVD_and_LRA()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stworzenie article_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_engine = ArticleSearchEngine(article_handler.dict_of_key_words)\n",
    "article_engine.create_search_engine()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testowanie poprawnego wyszukiwania dla stworzonej macierzy bez szumów i różnych *k*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: https://en.wikipedia.org/wiki/Mario_(singer) \ttitle: Mario (singer) \tprobability: 0.03348503068017055\n",
      "url: https://en.wikipedia.org/wiki/Colin_Linden \ttitle: Colin Linden \tprobability: 0.033465387176103006\n",
      "url: https://en.wikipedia.org/wiki/Sit_Down_Young_Stranger \ttitle: Sit Down Young Stranger \tprobability: 0.032961571198391375\n",
      "url: https://en.wikipedia.org/wiki/J._R._Richards \ttitle: J. R. Richards \tprobability: 0.032944445395590646\n",
      "url: https://en.wikipedia.org/wiki/Just_Another_Band_from_L.A. \ttitle: Just Another Band from L.A. \tprobability: 0.032850163454966684\n"
     ]
    }
   ],
   "source": [
    "article_engine.apply_SVD_and_LRA(k = 50)\n",
    "article_engine.find_best_articles(\"Colin Kendall Linden\", num_of_articles_to_return = 5, with_noise = False) # dla k = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: https://en.wikipedia.org/wiki/Colin_Linden \ttitle: Colin Linden \tprobability: 0.04605549892111489\n",
      "url: https://en.wikipedia.org/wiki/Yngwie_Malmsteen \ttitle: Fire &amp; Ice (Yngwie Malmsteen album) \tprobability: 0.042881796033845995\n",
      "url: https://en.wikipedia.org/wiki/Scott_Herren \ttitle: Crush the Sight-Seers \tprobability: 0.04183507412712115\n",
      "url: https://en.wikipedia.org/wiki/Still_Loving_You_(album) \ttitle: Still Loving You (album) \tprobability: 0.04102087375003581\n",
      "url: https://en.wikipedia.org/wiki/Time_Fades_Away \ttitle: Time Fades Away \tprobability: 0.04077065172128933\n"
     ]
    }
   ],
   "source": [
    "article_engine.apply_SVD_and_LRA(k = 100)\n",
    "article_engine.find_best_articles(\"Colin Kendall Linden\", num_of_articles_to_return = 5, with_noise = False) # dla k = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: https://en.wikipedia.org/wiki/Colin_Linden \ttitle: Colin Linden \tprobability: 0.08190366909591817\n",
      "url: https://en.wikipedia.org/wiki/Yngwie_Malmsteen \ttitle: Fire &amp; Ice (Yngwie Malmsteen album) \tprobability: 0.05257579049509905\n",
      "url: https://en.wikipedia.org/wiki/Still_Loving_You_(album) \ttitle: Still Loving You (album) \tprobability: 0.04926353180979251\n",
      "url: https://en.wikipedia.org/wiki/Mario_(singer) \ttitle: Mario (singer) \tprobability: 0.04652137779220739\n",
      "url: https://en.wikipedia.org/wiki/Scott_Herren \ttitle: Crush the Sight-Seers \tprobability: 0.044149888528352936\n"
     ]
    }
   ],
   "source": [
    "article_engine.apply_SVD_and_LRA(k = 200)\n",
    "article_engine.find_best_articles(\"Colin Kendall Linden\", num_of_articles_to_return = 5, with_noise = False) # dla k = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: https://en.wikipedia.org/wiki/Colin_Linden \ttitle: Colin Linden \tprobability: 0.4775091707717041\n",
      "url: https://en.wikipedia.org/wiki/Mick_Dalla-Vee \ttitle: Mick Dalla-Vee \tprobability: 0.09394976555523996\n",
      "url: https://en.wikipedia.org/wiki/Yngwie_Malmsteen \ttitle: Fire &amp; Ice (Yngwie Malmsteen album) \tprobability: 0.08288007669545847\n",
      "url: https://en.wikipedia.org/wiki/Sam_McDowell \ttitle: Sam McDowell \tprobability: 0.05945335293903401\n",
      "url: https://en.wikipedia.org/wiki/Time_Fades_Away \ttitle: Time Fades Away \tprobability: 0.05828665638924148\n"
     ]
    }
   ],
   "source": [
    "article_engine.apply_SVD_and_LRA(k = 500)\n",
    "article_engine.find_best_articles(\"Colin Kendall Linden\", num_of_articles_to_return = 5, with_noise = False) # dla k = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: https://en.wikipedia.org/wiki/Colin_Linden \ttitle: Colin Linden \tprobability: 0.5762730715956088\n",
      "url: https://en.wikipedia.org/wiki/Pink_Flag \ttitle: Pink Flag \tprobability: 0.0375870832186858\n",
      "url: https://en.wikipedia.org/wiki/Derek_and_Clive_Come_Again \ttitle: Derek and Clive Come Again \tprobability: 0.03711340074574137\n",
      "url: https://en.wikipedia.org/wiki/The_Two_of_Us_(1986_TV_series) \ttitle: The Two of Us (1986 TV series) \tprobability: 0.029137283524072084\n",
      "url: https://en.wikipedia.org/wiki/Alice_Mary_Robertson \ttitle: Alice Robertson \tprobability: 0.02339126615783837\n"
     ]
    }
   ],
   "source": [
    "article_engine.apply_SVD_and_LRA(k = 1000)\n",
    "article_engine.find_best_articles(\"Colin Kendall Linden\", num_of_articles_to_return = 5, with_noise = False) # dla k = 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testowanie poprawnego wyszukiwania dla macierzy podstawowej (zawiera szum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: https://en.wikipedia.org/wiki/Colin_Linden \ttitle: Colin Linden \tprobability: 0.5762730715956087\n",
      "url: https://en.wikipedia.org/wiki/Pink_Flag \ttitle: Pink Flag \tprobability: 0.03758708321868575\n",
      "url: https://en.wikipedia.org/wiki/Derek_and_Clive_Come_Again \ttitle: Derek and Clive Come Again \tprobability: 0.03711340074574138\n",
      "url: https://en.wikipedia.org/wiki/The_Two_of_Us_(1986_TV_series) \ttitle: The Two of Us (1986 TV series) \tprobability: 0.02913728352407209\n",
      "url: https://en.wikipedia.org/wiki/Alice_Mary_Robertson \ttitle: Alice Robertson \tprobability: 0.023391266157838462\n"
     ]
    }
   ],
   "source": [
    "article_engine.find_best_articles(\"Colin Kendall Linden\", num_of_articles_to_return = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: https://en.wikipedia.org/wiki/Colin_Linden \ttitle: Colin Linden \tprobability: 0.5762730715956087\n",
      "url: https://en.wikipedia.org/wiki/Pink_Flag \ttitle: Pink Flag \tprobability: 0.03758708321868575\n",
      "url: https://en.wikipedia.org/wiki/Derek_and_Clive_Come_Again \ttitle: Derek and Clive Come Again \tprobability: 0.03711340074574138\n",
      "url: https://en.wikipedia.org/wiki/The_Two_of_Us_(1986_TV_series) \ttitle: The Two of Us (1986 TV series) \tprobability: 0.02913728352407209\n",
      "url: https://en.wikipedia.org/wiki/Alice_Mary_Robertson \ttitle: Alice Robertson \tprobability: 0.023391266157838462\n",
      "url: https://en.wikipedia.org/wiki/Mick_Dalla-Vee \ttitle: Mick Dalla-Vee \tprobability: 0.016986558357453708\n",
      "url: https://en.wikipedia.org/wiki/Guadalupe_River_(Texas) \ttitle: Guadalupe River (Texas) \tprobability: 0.01613152623085068\n",
      "url: https://en.wikipedia.org/wiki/G-A-Y \ttitle: G-A-Y bar \tprobability: 0.013995954788215961\n",
      "url: https://en.wikipedia.org/wiki/Remain_in_Light \ttitle: Remain In Light \tprobability: 0.01224566477228362\n",
      "url: https://en.wikipedia.org/wiki/Fred_Hollows \ttitle: Frederick (Fred) Cossom Hollows \tprobability: 0.010916496723146133\n"
     ]
    }
   ],
   "source": [
    "article_engine.find_best_articles(\"Colin Kendall Linden\", num_of_articles_to_return = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: https://en.wikipedia.org/wiki/Hemolytic_disease_of_the_newborn \ttitle: Morbus haemolyticus neonatorum \tprobability: 0.11289337112420099\n",
      "url: https://en.wikipedia.org/wiki/Elders%27_Journal \ttitle: Elders' Journal \tprobability: 0.028457036092927983\n",
      "url: https://en.wikipedia.org/wiki/Playboy \ttitle: PlayBoy \tprobability: 0.027959744081795898\n",
      "url: https://en.wikipedia.org/wiki/Michalina_Wis%C5%82ocka \ttitle: Michalina Wisłocka \tprobability: 0.026432470036111967\n",
      "url: https://en.wikipedia.org/wiki/Stretch_marks \ttitle: Stretchmark \tprobability: 0.023472727032756054\n",
      "url: https://en.wikipedia.org/wiki/List_of_museum_ships \ttitle: Russian Museum.jpg \tprobability: 0.019700374217848156\n",
      "url: https://en.wikipedia.org/wiki/Bureau_of_Consular_Affairs \ttitle: Bureau of Consular Affairs \tprobability: 0.019171731961135984\n",
      "url: https://en.wikipedia.org/wiki/Valpara%C3%ADso_Region \ttitle: Región de Valparaíso \tprobability: 0.018338555673390773\n",
      "url: https://en.wikipedia.org/wiki/Djur%C3%B6_National_Park \ttitle: Djurö National Park \tprobability: 0.017227432474785077\n",
      "url: https://en.wikipedia.org/wiki/Classical_music \ttitle: Classical musicians by instrument \tprobability: 0.017197881925973827\n"
     ]
    }
   ],
   "source": [
    "article_engine.find_best_articles(\"HDFN include pregnancies maternal circulation\", num_of_articles_to_return = 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wnioski"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Wyszukiwarka znajduje poprawnie wyniki dla macierzy z szumami oraz bez\n",
    "- najlepszy wynik (subiektywnie) wyszukiwarka daje z macierzą bez szumów dla *k* = 1000 - im mniejszy zakres tym słabsza miara prawdopodobieństwa dla danych wektorów.\n",
    "- wyszukiwarka z szumami również podaje dobre wyniki, z szybszym działaniem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
